{
 "cells": [
  {
   "cell_type": "raw",
   "id": "338e954f-3a82-4a4e-ae92-f93753dc24c2",
   "metadata": {},
   "source": [
    "---\n",
    "author: \"Joy Wang\"\n",
    "title: \"Probabilty, clustering, linear algebra\"\n",
    "description: \"From uncertainty to structure: the hidden geometry of data\"\n",
    "date: \"20 October 2025\"\n",
    "categories: \n",
    "    - Book \n",
    "    - Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf1c24-6daf-4e7b-96d7-309c44415fca",
   "metadata": {},
   "source": [
    "## Chapter Recap: Chapters 4 – 6\n",
    "In these chapters, Ananthaswamy introduces probability and Bayesian thinking as tools for reasoning under uncertainty in machine learning. He connects probabilistic models and classification with ideas like likelihood, priors, and updating beliefs based on observed data. Of which there are groupings of similar data points, likely using clustering and dimensionality‑reduction ideas such as principal component analysis (PCA). It's how “similarity” can be formalized mathematically so machines can discover structure without explicit labels. And then there are matrices, the bane of my existence in MATH 1410. The linear algebra—matrices, eigenvectors, and transformations are the backbone of many learning algorithms. It explains how matrix operations compactly describe high‑dimensional computations in neural networks and other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba39b4b-29cb-4d11-8ed9-e56ef62e7fec",
   "metadata": {},
   "source": [
    "## My thoughts \n",
    "\n",
    "One of the things I struggle with in real life is uncertainty: not knowing which decision is best, not being sure what will happen next. Probability gives machines a structured way to handle exactly that feeling. When I think about probabilistic models, I imagine the machine as a careful bookkeeper of beliefs. It assigns degrees of belief to different possibilities and then updates those beliefs as new data arrives. Instead of insisting that something “is” or “is not” true, the machine says, “Given what I’ve seen so far, this explanation is more likely than that one.” That mindset changes how I look at data, too. It encourages me to see each new data point not as proof, but as another clue that shifts the odds in one direction or another.\n",
    "\n",
    "At the same time, there is this beautiful geometric side to learning that really appeals to me. When I read about clustering and dimensionality reduction, I imagine standing in a crowded room where everyone is speaking at once. My brain naturally groups similar voices and filters out noise; clustering algorithms do something similar for data. They measure similarity between points and pull together those that “sound alike,” forming natural groups even when nobody has labeled them in advance. Dimensionality reduction and matrices add another layer: they let the machine rotate, squish, and stretch data in high‑dimensional space so that important directions stand out and redundant ones fade into the background. I like to tell readers that, under the hood, machine learning is as much about reshaping spaces as it is about crunching numbers. Once I started visualizing data as clouds of points in a strange geometry, a lot of machine learning suddenly felt more intuitive."
   ]
  },
  {
   "cell_type": "raw",
   "id": "53033d76-a6e2-4956-a97a-f70eb663949b",
   "metadata": {},
   "source": [
    "![PCA](pca.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
