{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3d20a5f3-dbf8-44f0-87c4-e422338a00ff",
   "metadata": {},
   "source": [
    "---\n",
    "author: \"Joy Wang\"\n",
    "title: \"Kernels, physics, early neural networks\"\n",
    "description: \"When Straight Lines Aren’t Enough: Nonlinear Tricks in Machine Learning\"\n",
    "date: \"2 November 2025\"\n",
    "categories: \n",
    "    - Book \n",
    "    - Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c4a01-32d1-4a64-84bd-4e6d11f83581",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8973a-bd0a-4331-a0e5-36fa0268bcc7",
   "metadata": {},
   "source": [
    "## Chapter Recap: Chapters 7-9\n",
    "\n",
    "In these chapters, Ananthaswamy introduces kernel methods and support vector machines, showing how the “kernel trick” lets models handle non‑linear patterns. It does this by implicitly working in higher‑dimensional spaces, emphasizing the geometry behind separating data with hyperplanes after a clever transformation. \n",
    "\n",
    "![Kernel Trick Visualized](kerneltrick.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4296c8-7d36-498a-afc6-70ec19a4ef49",
   "metadata": {},
   "source": [
    "And with a little help from physics, we get to see networks from an energy‑based viewpoint. Hopfield networks and energy minimization illustrate how systems settle into low‑energy states that encode stored patterns and memories. Using the story of early neural‑network criticism (like limits of simple perceptrons), Ananthswamy explains why multilayer networks and new mathematical insights were needed. He also touches on universal approximation results, which show that even relatively simple neural networks can, in principle, represent very complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e1eed-4073-4acf-b540-71f0aeb5f0ec",
   "metadata": {},
   "source": [
    "## My thoughts\n",
    "\n",
    "We all know that real‑world data is very messy, as classes twist around each other, overlap, and form intricate shapes. The kernel methods mentioned give a way to push past that limitation. I think os it with a simple image: separating data points on a flat sheet of paper without straight lines. Now imagine I pick up parts of the paper and fold it into a complex 3D shape. Suddenly, a flat slice through that 3D shape can separate the classes that were hopelessly tangled before. Kernels let machines do that kind of folding in an abstract, high‑dimensional space without explicitly performing the folding. \n",
    "\n",
    "What I also find fascinating is how ideas from physics and early neural‑network debates shaped where we are now. Energy‑based models, for example, view learning as a kind of energy minimization: the system prefers low‑energy configurations that correspond to stable patterns or memories. That image of a landscape with valleys represents patterns connects with the optimization story from earlier chapters. Then there is the history of perceptrons and the criticism that almost “killed” neural networks for a while (how bizzare). I’m always struck by how a well‑founded critique (that single‑layer perceptrons can’t solve certain problems) led to a deeper probing. It's where the need for multilayer networks and better training methods arose. And the more I think about it, the history of AI learning feels very dramatized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
