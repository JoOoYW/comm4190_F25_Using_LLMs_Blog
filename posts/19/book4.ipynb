{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4c90477a-d472-42e5-8ec9-7941c52e1d48",
   "metadata": {},
   "source": [
    "---\n",
    "author: \"Joy Wang\"\n",
    "title: \"Backprop, vision, frontiers\"\n",
    "description: \"What makes deep neural networks trainable, and why do their quirks at scale still surprise researchers?\"\n",
    "date: \"25 November 2025\"\n",
    "categories: \n",
    "    - LLMs\n",
    "    - Book\n",
    "    - Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab1bce1-4ac7-46ca-a09f-f8ef108c1192",
   "metadata": {},
   "source": [
    "## Chapter Recap: Chapters 10-12\n",
    "\n",
    "It starts off focusing on backpropagation, explaining how the chain rule of calculus allows gradients to be computed efficiently in multilayer networks. Ananthaswamy shows how backprop and gradient descent together made modern deep learning practical by enabling large networks to learn from data. He then covers convolutional neural networks and their success in vision tasks such as object recognition and image classification. It explains how convolution, pooling, and layered feature extraction let machines automatically discover visual patterns like edges, textures, and shapes. \n",
    "\n",
    "![Convolutional neural networks visualization](cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b0119-5a01-4505-acfa-1602e4d492d2",
   "metadata": {},
   "source": [
    "He concludes with open questions and the frontier of machine learning, including phenomena like benign overfitting, double descent, and grokking (understand something intuitively or by empathy) in large models. It links these puzzling behaviors, and the rise of large language models, to the broader theme that much of modern AI operates in a still‑mysterious mathematical “unknown land.”"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49c977bc-f38e-4d81-b0f6-254b39cd6330",
   "metadata": {},
   "source": [
    "## My thoughts\n",
    "\n",
    "For a long time, deep neural networks were more of a dream than a practical tool, mainly because nobody knew how to adjust all those layers efficiently. Backpropagation is the piece that clicked it all into place for me. I see deep networks as long chains of decisions: each layer transforms the data a bit, and at the end the model either gets it right or wrong. Backprop uses the chain rule from calculus to trace that final error backward, figuring out how much each earlier layer contributed to the mistake. Of couse, there's the error signal flowing backward through the network like water through pipes, leaving behind tiny adjustments at every valve (weight) it passes. It's fascinating and the reason why deep learning exploded, there was now a systematic way to update millions or billions of parameters in a coordinated way.\n",
    "\n",
    "Here, convolutional neural networks see images much differently, first as tiny patches and edges, then as motifs and textures, and finally as full objects. But then the frontier chapters remind everyone that, even with all this intuition, large models still behave in ways that surprise everyone. With double descent curves, grokking, and bizarre generalization patterns that defy simple explanations we're slowly trying to reverse‑engineer why they work as well as they do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
