{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f10dd7e5-050d-43cd-a71f-ef0582a4dbe6",
   "metadata": {},
   "source": [
    "---\n",
    "author: \"Joy Wang\"\n",
    "title: \"Patterns, numbers, and optimization\"\n",
    "description: \"How do machines turn patterns into predictions?\"\n",
    "date: \"25 November 2025\"\n",
    "categories: \n",
    "    - Book \n",
    "    - Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecac04-5327-4198-87c4-23cc429c51f5",
   "metadata": {},
   "source": [
    "## Chapters 1-3\n",
    "Ananthaswamy introduces pattern recognition as the essence of machine learning, using animal imprinting (like ducklings) to motivate how both brains and machines learn from data. It then presents Rosenblatt’s perceptron as an early learning machine, explaining weights, bias, linear decision boundaries, and the basic idea of supervised learning. He shows how real‑world entities are translated into numerical vectors so machines can process them, emphasizing feature representation and high‑dimensional spaces. It introduces the language of linear models and regression, setting up later knowledge that rely on vectors, inner products, and geometric intuition. What's more intriguin is his analogy of \"a marble in a bowl\". Optimization and gradiet descent rely on the method of learning as rolling downhill on an error surface to minimize loss. Historical work on adaptive filters and Widrow–Hoff’s LMS algorithm shows how iterative updates make models gradually improve their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55ef58-3c91-4cf1-8ba1-ae93f18a3bf2",
   "metadata": {},
   "source": [
    "## My thoughts \n",
    "\n",
    "Machines start by representing inputs as numbers, then use very simple rules to adjust those numbers until they can consistently separate one kind of example from another. The perceptron and gradient descent illustrate this: each time the machine makes a mistake, it nudges its internal weights in the direction that would have reduced the error, and over many such nudges the model effectively “rolls downhill” on an error landscape toward a configuration that captures the pattern in the data.\n",
    "\n",
    "At its core, machine learning is about turning messy, real‑world data into reliable predictions by gradually uncovering patterns hidden in numbers. The perceptron is a perfect starting point because it is both simple and surprisingly powerful: it takes inputs (like pixel values or measurements), multiplies them by adjustable weights, adds them up, and decides which side of a threshold the result falls on. When the perceptron gets an example wrong, it updates its weights in the direction that would have pushed the prediction toward the correct answer next time. Over many such updates, the model carves out a boundary in the data space that separates one class from another. Even though this sounds mechanical, it captures a key intuition: learning can emerge from a basic rule of “nudge toward less error.”\n",
    "\n",
    "Gradient descent generalizes this idea from a simple perceptron to a wide range of models. Instead of thinking about one weight at a time, gradient descent imagines an error landscape defined over all possible settings of all the model’s parameters. Each point in this landscape represents a different version of the model, and the height at that point represents how badly that version is performing. Learning is then like dropping a marble on this landscape and letting it roll downhill. At each step, the gradient tells the model which direction decreases the error most steeply, and the learning rate controls how big a step it takes. What looks from the outside like magic—an algorithm that suddenly recognizes handwritten digits or predicts housing prices—is, on the inside, thousands or millions of tiny, local adjustments that collectively push the model toward a configuration that captures the underlying patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40287b4-cc2d-41b8-8c2b-cd0919685960",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
