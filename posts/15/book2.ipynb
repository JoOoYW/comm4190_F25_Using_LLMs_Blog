{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f10dd7e5-050d-43cd-a71f-ef0582a4dbe6",
   "metadata": {},
   "source": [
    "---\n",
    "author: \"Joy Wang\"\n",
    "title: \"Patterns, numbers, and optimization\"\n",
    "description: \"How do machines turn patterns into predictions?\"\n",
    "date: \"6 October 2025\"\n",
    "categories: \n",
    "    - Book \n",
    "    - Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecac04-5327-4198-87c4-23cc429c51f5",
   "metadata": {},
   "source": [
    "## Chapter Recap: Chapters 1-3\n",
    "Ananthaswamy introduces pattern recognition as the essence of machine learning, using animal imprinting (like ducklings) to motivate how both brains and machines learn from data. It then presents Rosenblatt’s perceptron as an early learning machine, explaining weights, bias, linear decision boundaries, and the basic idea of supervised learning. He shows how real‑world entities are translated into numerical vectors so machines can process them, emphasizing feature representation and high‑dimensional spaces. It introduces the language of linear models and regression, setting up later knowledge that rely on vectors, inner products, and geometric intuition. What's more intriguin is his analogy of \"a marble in a bowl\". Optimization and gradiet descent rely on the method of learning as rolling downhill on an error surface to minimize loss. Historical work on adaptive filters and Widrow–Hoff’s LMS algorithm shows how iterative updates make models gradually improve their predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "477556c5-1134-4ae5-8a3e-bdbce5f081c9",
   "metadata": {},
   "source": [
    "## My thoughts \n",
    "\n",
    "When I first started learning about machine learning, I assumed it required incredibly fancy math right from the start. What surprised me is how far you can get with a very simple idea: notice a pattern, make a guess, see how wrong you were, then nudge your internal “settings” to do a little better next time. That is essentially what the perceptron and gradient descent are doing. I like to picture a machine staring at a pile of labeled examples—say, images tagged “cat” or “not cat.” Each example gets turned into a list of numbers, and the machine learns a rule (a line or plane in some abstract space) that tries to separate the cat points from the non‑cat points. Every time the rule misclassifies an example, the machine tweaks its weights so that the misclassified point moves closer to the correct side of the boundary. Over many iterations, those tiny corrections accumulate into something that suddenly looks like “intelligence”: the machine starts getting new examples right.\n",
    "\n",
    "At its core, machine learning is about turning messy, real‑world data into reliable predictions by gradually uncovering patterns hidden in numbers. To many, the perceptron is a perfect starting point because it is both simple and surprisingly powerful: it takes inputs (like pixel values or measurements), multiplies them by adjustable weights, adds them up, and decides which side of a threshold the result falls on. When the perceptron gets an example wrong, it updates its weights in the direction that would have pushed the prediction toward the correct answer next time. Over several updates, the model carves out a boundary in the data space that separates one class from another. Even though to me, this sounds mechanical, it captures a key intuition that learning can emerge from a basic rule of “nudge toward less error.”"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9db2f9d4-4c16-4f79-b063-29d5c0b8223b",
   "metadata": {},
   "source": [
    "![Visualization of a perceptron](percepton.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f55ef58-3c91-4cf1-8ba1-ae93f18a3bf2",
   "metadata": {},
   "source": [
    "For me, the most powerful mental image from these early chapters is the “bowl” of the loss landscape. I picture myself dropping a marble (the current model) into a bumpy bowl (the error surface). The marble rolls downhill, following the steepest direction, which is what the gradient tells us. In practice, this looks like repeatedly adjusting the parameters in the direction that most quickly reduces error. What I love about this story is how human it feels: we, too, learn by trial and error. The only difference is that a machine can keep perfect score, compute exactly how each mistake should change its internal settings, and repeat that process millions of times without getting bored. When I frame machine learning this way for readers, they often realize that “learning from data” is less mysterious than it sounds—just a disciplined, algorithmic form of the same pattern‑spotting we rely on every day. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
