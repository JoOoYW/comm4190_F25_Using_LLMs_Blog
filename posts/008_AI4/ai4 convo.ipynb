{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d307d0cd-0e29-449b-a633-3efb33cd8c06",
   "metadata": {},
   "source": [
    "---\n",
    "author: \"Joy Wang\" \n",
    "title: \"On our way to trip up a robot!\"\n",
    "description: \"With arbitrary requests to prove AI will NOT take over the world, many don't really understand how language models actually work.\"\n",
    "date: \"7 October 2025\"\n",
    "categories: \n",
    "    - AI\n",
    "    - Testing \n",
    "    - Chatbots\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd9bf37f-c6ac-4062-99ec-88bd33551394",
   "metadata": {},
   "source": [
    "![Strawberry](strawberry r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31354828-9430-4e57-87c0-17d5eeef21b9",
   "metadata": {},
   "source": [
    "In 2024, a [query](https://community.openai.com/t/incorrect-count-of-r-characters-in-the-word-strawberry/829618) circulated around the internet of someone asking a simple question to ChatGPT, \"How many 'r's' in 'strawberry'\". The answer? Two, according to GPT-4o. Now how could that be? Where did ChatGPT go wrong? Well, it's not where did it go wrong, but how much effort people put to boldly claim AI won't take over jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c2bd4-d341-492a-8985-3c1ad1a41739",
   "metadata": {},
   "source": [
    "ChatGPT and other LLM's are based off of tokenization, taking the whole prompt and chunking it down to an average of four characters per token. Based on all of the values and weightings, it then passes them through different parameters (and a small amount of randomness), output whatever it thinks best matches your request. That means, it can't directly count the number of letters in a word. For a word like Mississippi which can be broken down into {4I, 2P, 4S, 1M}, LLM's may see it as {1MISS, 1ISS, 1PPI}. So in the case of straberry, it's a token boundary issue where because the intervals of counting characters are determined in a tricky way, the model lacks an internal letter-level representation. One 'r' might be at the end of one token and two 'r's in the middle of the next token), thus fumbling the count."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e738c65-15ee-437d-a75e-c0a390faaa09",
   "metadata": {},
   "source": [
    "![Example of how a prompt is broken down](token.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194eb1f-837e-4f92-a442-25c2d299e4db",
   "metadata": {},
   "source": [
    "Instead of spelling, let's try a math problem: If x and y are the tens digit and the units (ones) digit, respectively, of the product 725,278 * 67,066, what is the value of x + y? The solution is quite easy to figure out, simply take 8 and 6 from the ones, multiply them together, and add up the digits of their product: 8 * 6 = 48 â†’ 4 + 8 = 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a61595c-eec0-4b89-9a2d-26d0b999dc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 48641494348\n"
     ]
    }
   ],
   "source": [
    "print(725278 * 67066)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2839f975-ecc5-4a22-9ad9-2dfd1cf709d1",
   "metadata": {},
   "source": [
    "I highly suggest using [the tokenizer website](https://huggingface.co/spaces/Xenova/the-tokenizer-playground) to see how GPT splits the prompt into tokens!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R (COMM2100)",
   "language": "R",
   "name": "r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
