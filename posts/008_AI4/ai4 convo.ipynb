{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d307d0cd-0e29-449b-a633-3efb33cd8c06",
   "metadata": {},
   "source": [
    "---\n",
    "author: \"Joy Wang\" \n",
    "title: \"On our way to trip up a robot!\"\n",
    "description: \"With arbitrary requests to prove AI will NOT take over the world, many don't really understand how language models actually work.\"\n",
    "date: \"7 October 2025\"\n",
    "categories: \n",
    "    - AI\n",
    "    - Testing \n",
    "    - Chatbots\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd9bf37f-c6ac-4062-99ec-88bd33551394",
   "metadata": {},
   "source": [
    "![Strawberry](strawberry r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31354828-9430-4e57-87c0-17d5eeef21b9",
   "metadata": {},
   "source": [
    "## Prompting \n",
    "\n",
    "In 2024, a [query](https://community.openai.com/t/incorrect-count-of-r-characters-in-the-word-strawberry/829618) circulated around the internet of someone asking a simple question to ChatGPT, \"How many 'r's' in 'strawberry'\". The answer? Two, according to GPT-4o. Now how could that be?  Are we really sure AI will take over jobs in the future? Where did ChatGPT go wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c2bd4-d341-492a-8985-3c1ad1a41739",
   "metadata": {},
   "source": [
    "ChatGPT and other LLM's are based off of tokenization, the process of taking the whole prompt and chunking it down to an average of four characters per token. They then turn those gorups of characters into token IDs, and based on all of the values and weightings, it then passes them through different parameters (and a small amount of randomness), output whatever it thinks best matches your request. For something to iterate over thousands of words, it uses up a lot of energy and natural resources. And that means it can't directly count the number of letters in a word. For a word like Mississippi which can be broken down into {4I, 2P, 4S, 1M}, LLM's may see it as <span style = 'color: #cd93e6'>MISS</span><span style = 'color: #93e6c7'>ISSIPPI</span>. So in the case of strawberry, it's a token boundary issue where because the intervals of counting characters are determined in a tricky way, the model lacks an internal letter-level representation. One 'r' might be at the end of one token and two 'r's in the middle of the next token), thus fumbling the count."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e738c65-15ee-437d-a75e-c0a390faaa09",
   "metadata": {},
   "source": [
    "![Example of how a prompt is broken down](token.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194eb1f-837e-4f92-a442-25c2d299e4db",
   "metadata": {},
   "source": [
    "## Arithmetic \n",
    "\n",
    "Instead of spelling, let's try a math problem: If x and y are the tens digit and the units (ones) digit, respectively, of the product 725,278 * 67,066, what is the value of x + y? The solution is quite easy to figure out, simply take 8 and 6 from the ones, multiply them together, and add up the digits of their product: 8 * 6 = 48 â†’ 4 + 8 = 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a61595c-eec0-4b89-9a2d-26d0b999dc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 48641494348\n"
     ]
    }
   ],
   "source": [
    "print(725278 * 67066)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d75a4-f03e-4aba-bd0b-523e79a57170",
   "metadata": {},
   "source": [
    "Contrary to popular beliefs, ChatGPT and many other LLMs are unable to do arithmetic like a computer's calculator program. Rather, they generate text based on learned patterns, which is why it can produce errors, particularly with tasks that require precise, deterministic calculations. Again, they rely on tokens which can break up the digits in a way that would produce an error, such as 168812*980/456-9533699 becoming <span style='color:#cd93e6'>168</span><span style='color:#93e6c7'>812</span><span style='color:#fcc358'>&ast;</span><span style='color:#e37b74'>980</span><span style='color:#7fa0e3'>/</span><span style='color:#cd93e6'>456</span><span style='color:#93e6c7'>-</span><span style='color:#fcc358'>953</span><span style='color:#e37b74'>369</span><span style='color:#7fa0e3'>9</span>. If you do rely on ChatGT or Gemini for any sort of problem set, always double check the calculations on your own. In fact, it would be more reliable to use a specialized tool or algorithm that explicitly performs calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2839f975-ecc5-4a22-9ad9-2dfd1cf709d1",
   "metadata": {},
   "source": [
    "I highly suggest using [the tokenizer website](https://huggingface.co/spaces/Xenova/the-tokenizer-playground) to see how GPT splits the prompt into tokens!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R (COMM2100)",
   "language": "R",
   "name": "r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
